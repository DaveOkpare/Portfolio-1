{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Rundown with the Movie Review Db\n",
    "\n",
    "http://fastml.com/classifying-text-with-bag-of-words-a-tutorial/\n",
    "\n",
    "#### Go Linear for Bag of Words.\n",
    "\n",
    "This is because you will be dealing with sparse and high dimensional data. Methods like random Forest will be very prone to overfitting in this situation. Linear models train fast, add a layer of simplicity, and will not overfit the very sparse data.\n",
    "\n",
    "#### TFIDF Vectorizer reduces noise\n",
    "TFIDF is a scaling algorithm that adjust your bag of words to deemphasize frequently use words. The purpose of doing this is to have your model focus on only those words that might be distinguishing characteristics of certain types of documents.\n",
    "\n",
    "#### Stop Words\n",
    "Predefined set of common words in any language that provide little substance to the meaning of the document. Cutting them  from your bag of words reduces dimensionality and noise of your data resulting in a more accurate model. Removal of stop words must be cross-validated.\n",
    "\n",
    "#### n-grams\n",
    "Do not use stop words if you are using n-grams. puts n number of sequential words in your bag. Good for inferring meaning.\n",
    "\n",
    "### Dimensionality and Memory\n",
    "Linear models are preferred with bag of word because  d>>n where d is dimension and n is number of samples. Regular TFIDF Vectorizer will need a lot of memory, instead you could try HashingVectorizer and online learning to create a model that is less accurate, trains quicker, and is less memory intensive.\n",
    "\n",
    "### RNNs vs Linear Models.\n",
    "For smaller datasets in sentiment analysis tasks, linear models with n-grams will out perform RNNs. When the number of samples grows to 100k+ RNNs start to outperform linear models.\n",
    "\n",
    "https://arxiv.org/abs/1412.5335"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn Text Feature Extraction\n",
    "\n",
    "http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.feature_extraction.text.HashingVectorizer\n",
    "Converts a np collection of text documents to a matrix of token occurences. (token is string name knowing idx column)\n",
    "\n",
    "\n",
    "Pros:\n",
    "\n",
    "- low memory scalable for large datasets, no need to store vocab ditionary in memory.\n",
    "- can be used for partial_fit learning methods (online learning)\n",
    "\n",
    "Cons: \n",
    "\n",
    "- No inverse transform (matrix indices to string names) Less useful for figuring out which words are characteristic of different classes.\n",
    "- Therre can be hashing collisions. (not a problem if hyperparameters are set reeally high 2^18)\n",
    "- No TFIDF Weighting , need uniform weighting of all words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\n",
    "\n",
    "A generative statistical model (unsupervised algorithm) that allows the inference of unobserved groups that explain why some samples in the data are similar to others. Topic modelling in NLP.\n",
    "\n",
    "Each document is a collection of smaller topics, and each words use in the document is attributed to one of the documen's topics.\n",
    "\n",
    "### Topics\n",
    "Each document is viewed as a mixture of different topics. LDA assigns topics to each document. Assumes that documents cover a small set of topics and topics contain a small number of frequently used terms.\n",
    "\n",
    "A topic has probabilities of generating various words, whereas words without special relevance will have a even probability between classes.\n",
    "\n",
    "Topics are identified on the basis of detection of the likelihood of term cooccurence. (ie some words are used more frequently together)\n",
    "\n",
    "### Inference\n",
    "\n",
    "The model learns topic word probability, topic of each word, and the particular topic mixture of each document through bayesian inference. IE given that these words appeared together what is the probability of the topic being X?\n",
    "\n",
    "### Takeaway\n",
    "\n",
    "Each document is analyzed in a bag of words perspective and the probability distribution of the words contained. Topics are created by detecting clusters of words that frequently cooccur in documents. Each topic has its own distribution of words contained.  Match the document distibution to the various topic distributions we found, and pick the best fits. The best fitting topics will be assigned as the topics of the document."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
