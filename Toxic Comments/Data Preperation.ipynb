{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "The following material is inspired by jagangupta in the following link. I will be using his function to remove peices of text that might lead to overfitting. (ip address, usernames, links) If the model caught on that a particular ip was regularly a toxic poster, it might learn a decision rule that might not apply well to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "train = pd.read_csv('C:\\\\Users\\\\harri\\\\.kaggle\\\\competitions\\\\jigsaw-toxic-comment-classification-challenge\\\\train.csv\\\\train.csv').fillna(' ')\n",
    "test = pd.read_csv('C:\\\\Users\\\\harri\\\\.kaggle\\\\competitions\\\\jigsaw-toxic-comment-classification-challenge\\\\test.csv\\\\test.csv').fillna(' ')\n",
    "\n",
    "X_train = train['comment_text']\n",
    "X_test = test['comment_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Examples\n",
    "\n",
    "Let's print the first 10 examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(0, 10):\n",
    "    print(X_train[i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=train.iloc[:,2:].sum()\n",
    "#Class Occurence Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "ax= sns.barplot(x.index, x.values, alpha=0.8)\n",
    "plt.title(\"# per Class\")\n",
    "plt.ylabel('# of Occurrences', fontsize=12)\n",
    "plt.xlabel('Type ', fontsize=12)\n",
    "#adding the text labels\n",
    "rects = ax.patches\n",
    "labels = x.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud Visualization: Toxic Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "\n",
    "toxic_subset = train[train.toxic==1]\n",
    "text = toxic_subset.comment_text.values\n",
    "wc = WordCloud(background_color = \"black\", max_words = 2000,stopwords = stopword)\n",
    "wc.generate(\"\".join(text))\n",
    "\n",
    "#show wordcloud\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Frequencies: Toxic Comments\", fontsize = 20)\n",
    "plt.imshow(wc.recolor(colormap =\"viridis\", alpha = 0.95))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([train.iloc[:,0:2], test.iloc[:,0:2]])\n",
    "df = df.reset_index(drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Unique word count\n",
    "df['count_unique_word']=df[\"comment_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "#punctuation count\n",
    "df[\"count_punctuations\"] =df[\"comment_text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "upper case words count\n",
    "df[\"count_words_upper\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Function\n",
    "Cleaning out leaky features and preprocessing our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy \n",
    "\n",
    "\n",
    "def clean(comment):\n",
    "    nlp = spacy.load('en_cor_web_md')\n",
    "    #conv to lowercase\n",
    "    comment = comment.lower()\n",
    "    #replace new line\n",
    "    comment = re.sub('\\\\n','',comment)\n",
    "    #remove ip \n",
    "    comment = re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\", \"\",comment)\n",
    "    #remove username\n",
    "    comment=re.sub(\"\\[\\[.*\\]\",\"\",comment)\n",
    "    #remove urls\n",
    "    comment = re.sub(\"http://.*com\", '', comment)\n",
    "    #article ids\n",
    "    comment = re.sub(\"\\d:\\d\\d\\s{0,5}$\", '', comment)\n",
    "    \n",
    "    #run spacy pipeline on comment for tokenization and lemmatization.\n",
    "    doc = nlp(comment)\n",
    "    comment = ''\n",
    "    for token in doc:\n",
    "        comment = comment.join(token.lemma_ + ' ')\n",
    "    return comment\n",
    "\n",
    "df['comment_text'] = df['comment_text'].apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=200,  max_features=10000, \n",
    "            strip_accents='unicode', analyzer='word',ngram_range=(1,2),\n",
    "            use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "tfidf.fit(df.comment_text)\n",
    "\n",
    "train =  tfidf.transform(df[:train.shape[0],  1])\n",
    "test = tfidf.transform(df[train.shape[0]:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_csv('C:\\\\Users\\\\harri\\\\.kaggle\\\\competitions\\\\jigsaw-toxic-comment-classification-challenge\\\\train.csv\\\\clean_train.csv')\n",
    "test.to_csv('C:\\\\Users\\\\harri\\\\.kaggle\\\\competitions\\\\jigsaw-toxic-comment-classification-challenge\\\\train.csv\\\\clean_test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
